{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## e2eET Skeleton Based HGR Using Data-Level Fusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2N5i0LEcxRtT"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pickle\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _resize_gestures(in_gest_seqs, target_length=250):\n",
        "    \"\"\"Resize the time series by interpolating them to the same length\"\"\"\n",
        "\n",
        "    out_gest_seqs = []\n",
        "    for sequence in in_gest_seqs:\n",
        "        zoomed_skeletons = []\n",
        "        for skeleton in range(np.size(sequence, 1)):\n",
        "            _zoom_skel = ndimage.zoom(sequence.T[skeleton], target_length / len(sequence), mode=\"reflect\")\n",
        "            zoomed_skeletons.append(_zoom_skel)\n",
        "\n",
        "        out_gest_seqs.append(np.array(zoomed_skeletons).T)\n",
        "\n",
        "    return np.array(out_gest_seqs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _write_data(data, filepath):\n",
        "    \"\"\"Save the dataset to a file. Note: data is a dict with keys 'X_train', ...\"\"\"\n",
        "\n",
        "    with open(filepath, \"wb\") as output_file: pickle.dump(data, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_txt_gestures(type, resize_length, root):\n",
        "    \"\"\"\n",
        "    Get the 3D pose gestures sequences, and their associated labels.\n",
        "    Output:  a tuple of (gestures, str_labels, labels_14, labels_28).\n",
        "    \"\"\"\n",
        "\n",
        "    # _____\n",
        "    assert \"DHG1428\" in root, \"Check that the correct dataset folder is provided!\"\n",
        "    assert type in [\"2d\", \"3d\"], \"Data type has to be specified ['2d' / '3d']\"\n",
        "\n",
        "    # _____\n",
        "    translation = {\n",
        "        \"gesture_1\": \"Grab-\",\n",
        "        \"gesture_2\": \"Tap-\",\n",
        "        \"gesture_3\": \"Expand-\",\n",
        "        \"gesture_4\": \"Pinch-\",\n",
        "        \"gesture_5\": \"Rotation_CW-\",\n",
        "        \"gesture_6\": \"Rotation_CCW-\",\n",
        "        \"gesture_7\": \"Swipe_Right-\",\n",
        "        \"gesture_8\": \"Swipe_Left-\",\n",
        "        \"gesture_9\": \"Swipe_Up-\",\n",
        "        \"gesture_10\": \"Swipe_Down-\",\n",
        "        \"gesture_11\": \"Swipe_X-\",\n",
        "        \"gesture_12\": \"Swipe_+-\",  # swapped\n",
        "        \"gesture_13\": \"Swipe_V-\",  # swapped\n",
        "        \"gesture_14\": \"Shake-\",\n",
        "    }\n",
        "    trash = [\n",
        "        root,\n",
        "        \"skeleton_world.txt\",\n",
        "        \"skeleton_image.txt\",\n",
        "        \"\\\\\",\n",
        "        \"_\",\n",
        "        \"inger\",\n",
        "        \"ubject\",\n",
        "        \"ssai\",\n",
        "    ]\n",
        "    pattern = (\n",
        "        root + \"/gesture_*/finger_*/subject_*/essai_*/skeleton_world.txt\" # 3D\n",
        "        if type == \"3d\"\n",
        "        else root + \"/gesture_*/finger_*/subject_*/essai_*/skeleton_image.txt\" # 2D\n",
        "    )\n",
        "    \n",
        "    # _____\n",
        "    filenames = sorted(glob.glob(pattern))\n",
        "    gestures = [np.genfromtxt(f) for f in filenames]\n",
        "    if resize_length: gestures = _resize_gestures(gestures, target_length=resize_length)\n",
        "\n",
        "    # _____\n",
        "    labels_14 = [int(f.split(\"\\\\\")[-5].split(\"_\")[1]) for f in filenames]\n",
        "    n_fingers_used = [int(f.split(\"\\\\\")[-4].split(\"_\")[1]) for f in filenames]\n",
        "    labels_28 = [\n",
        "        labels_14[idx] if n_fingers == 1 else 14 + labels_14[idx]\n",
        "        for idx, n_fingers in enumerate(n_fingers_used)\n",
        "    ]\n",
        "\n",
        "    # _____\n",
        "    str_labels = filenames.copy()\n",
        "    for i in range(len(str_labels)):\n",
        "        gesture_n = str_labels[i].split(\"\\\\\")[-5]\n",
        "        str_labels[i] = str_labels[i].replace(gesture_n, translation[gesture_n])\n",
        "        for subs in trash:\n",
        "            str_labels[i] = str_labels[i].replace(subs, \"\")\n",
        "\n",
        "    assert len(str_labels) == len(labels_14) == len(labels_28)\n",
        "    return gestures, str_labels, labels_14, labels_28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_pckl_data(filepath):\n",
        "    \"\"\"\n",
        "    Returns hand gesture sequences (X) and their associated labels (Y).\n",
        "    Each sequence has three different labels: str_labels, labels_14, and labels_28.\n",
        "    \"\"\"\n",
        "\n",
        "    file = open(filepath, \"rb\")\n",
        "    data = pickle.load(file, encoding=\"latin1\") # change to 'latin1' to 'utf8' if the data does not load\n",
        "    file.close()\n",
        "\n",
        "    return (\n",
        "        data[\"X_train\"], data[\"X_valid\"],\n",
        "        data[\"train_str_labels\"], data[\"valid_str_labels\"],\n",
        "        data[\"y_train_14\"], data[\"y_valid_14\"],\n",
        "        data[\"y_train_28\"], data[\"y_valid_28\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_train_valid_data(type, root, resize_length=None, seed=17711, save_path=None):\n",
        "    assert type in [\"2d\", \"3d\"], \"Data type has to be specified ['2d' / '3d']\"\n",
        "    \n",
        "    # load the dataset gesture sequnences from file(s)\n",
        "    gestures, str_labels, labels_14, labels_28 = load_txt_gestures(type, resize_length, root)\n",
        "    print(\">>> <gestures, str_labels, labels_14, labels_28> loaded successfully!\")\n",
        "    \n",
        "    # split into train and validation subsets \n",
        "    (\n",
        "        X_train, X_valid,\n",
        "        train_str_labels, valid_str_labels,\n",
        "        y_train_14, y_valid_14,\n",
        "        y_train_28, y_valid_28,\n",
        "    ) = train_test_split(gestures, str_labels, labels_14, labels_28, test_size=0.30, random_state=seed)\n",
        "    print(f\">>> {type} training ({X_train.shape}) and validation ({X_valid.shape}) data created.\")\n",
        "    \n",
        "    # save the test-train data to disk\n",
        "    if save_path is None: save_path = \"../datasets\"\n",
        "    save_path = f\"{save_path}/DHG1428_{type}_dictTVS_l{resize_length}_s{len(gestures)}.pckl\"\n",
        "        \n",
        "    data = {\n",
        "        \"X_train\": X_train, \"X_valid\": X_valid,\n",
        "        \"train_str_labels\": train_str_labels, \"valid_str_labels\": valid_str_labels,\n",
        "        \"y_train_14\": y_train_14, \"y_valid_14\": y_valid_14,\n",
        "        \"y_train_28\": y_train_28, \"y_valid_28\": y_valid_28,\n",
        "    }\n",
        "    _write_data(data, filepath=save_path)\n",
        "    print(f\">>> TVS train-valid data written to <{save_path}> successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> <gestures, str_labels, labels_14, labels_28> loaded successfully!\n",
            ">>> 3d training ((1960, 250, 66)) and validation ((840, 250, 66)) data created.\n",
            ">>> TVS train-valid data written to <../datasets/DHG1428_3d_dictTVS_l250_s2800.pckl> successfully!\n"
          ]
        }
      ],
      "source": [
        "create_train_valid_data(type=\"3d\", root=\"../datasets/DHG1428\", resize_length=250)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "deep_learning_hand_gesture_recognition_01_data_download.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 ('hlu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "fd523941f93840f3066dc30de95cef5f0b3787e3e58410dcba83902e24288dc0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
