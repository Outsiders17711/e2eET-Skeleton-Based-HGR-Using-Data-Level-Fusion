{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBU Kinect Interaction Dataset v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skeleton_txts(ds_directory=\"../datasets/SBUKId/\", debug=False, cleanup=False):\n",
    "    ds_directory = Path(ds_directory)\n",
    "    txts_directory = ds_directory.parent.joinpath(f\"{ds_directory.stem}.txts/\")\n",
    "    txts_directory.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    sets_transformation = {\n",
    "        \"s01s02\": \"st01-p1p2\",\n",
    "        \"s01s03\": \"st02-p1p3\",\n",
    "        \"s01s07\": \"st03-p1p7\",\n",
    "        \"s02s01\": \"st04-p2p1\",\n",
    "        \"s02s03\": \"st05-p2p3\",\n",
    "        \"s02s06\": \"st06-p2p6\",\n",
    "        \"s02s07\": \"st07-p2p7\",\n",
    "        \"s03s02\": \"st08-p3p2\",\n",
    "        \"s03s04\": \"st09-p3p4\",\n",
    "        \"s03s05\": \"st10-p3p5\",\n",
    "        \"s03s06\": \"st11-p3p6\",\n",
    "        \"s04s02\": \"st12-p4p2\",\n",
    "        \"s04s03\": \"st13-p4p3\",\n",
    "        \"s04s06\": \"st14-p4p6\",\n",
    "        \"s05s02\": \"st15-p5p2\",\n",
    "        \"s05s03\": \"st16-p5p3\",\n",
    "        \"s06s02\": \"st17-p6p2\",\n",
    "        \"s06s03\": \"st18-p6p3\",\n",
    "        \"s06s04\": \"st19-p6p4\",\n",
    "        \"s07s01\": \"st20-p7p1\",\n",
    "        \"s07s03\": \"st21-p7p3\",\n",
    "    }\n",
    "\n",
    "    # ---\n",
    "    for f in ds_directory.rglob(\"*.txt\"):\n",
    "        _set, _activity, _sequence = f.parts[3:-1]\n",
    "        n_f = txts_directory.joinpath(f\"{sets_transformation[_set]}-a{int(_activity):02}-sq{int(_sequence):02}.txt\")\n",
    "        f.replace(n_f)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"{f=} \\n{n_f = }\")\n",
    "            break\n",
    "\n",
    "    # ---\n",
    "    l_dataset = len(list(txts_directory.iterdir()))\n",
    "    print(f\"<{l_dataset=}> skeleton text files extracted to <{txts_directory}> dataset directory.\\n\")\n",
    "    \n",
    "    shutil.rmtree(path=ds_directory, ignore_errors=True) if cleanup else None\n",
    "    return str(txts_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions reused from outsiders17711-3d-dynamic-hgr parse-data-*.ipynb\n",
    "# ---\n",
    "def _resize_gestures(in_gest_seqs, target_length=250):\n",
    "    out_gest_seqs = []\n",
    "    for sequence in in_gest_seqs:\n",
    "        zoomed_skeletons = []\n",
    "        for skeleton in range(np.size(sequence, 1)):\n",
    "            _zoom_skel = ndimage.zoom(sequence.T[skeleton], target_length / len(sequence), mode=\"reflect\")\n",
    "            zoomed_skeletons.append(_zoom_skel)\n",
    "\n",
    "        out_gest_seqs.append(np.array(zoomed_skeletons).T)\n",
    "\n",
    "    return np.array(out_gest_seqs)\n",
    "\n",
    "# ---\n",
    "def _write_data(data, filepath):\n",
    "    with open(filepath, \"wb\") as output_file: pickle.dump(data, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt_actions(txts_directory=\"../datasets/SBUKId.txts/\", resize_actions=True):\n",
    "\n",
    "    activity_labels = {\n",
    "        \"a01\": \"1.Approaching\",\n",
    "        \"a02\": \"2.Departing\",\n",
    "        \"a03\": \"3.Kicking\",\n",
    "        \"a04\": \"4.Pushing\",\n",
    "        \"a05\": \"5.ShakingHands\",\n",
    "        \"a06\": \"6.Hugging\",\n",
    "        \"a07\": \"7.Exchanging\",\n",
    "        \"a08\": \"8.Punching\",\n",
    "    }\n",
    "\n",
    "    # ---\n",
    "    files = list(Path(txts_directory).rglob(\"*.txt\"))\n",
    "    actions = np.array([np.genfromtxt(f, delimiter=\",\")[:, 1:] for f in files], dtype=object)\n",
    "    action_lengths = np.array([len(a) for a in actions])\n",
    "    print(f\"{action_lengths.min()=}, {action_lengths.mean()=:.1f}, {action_lengths.max()=}\")\n",
    "\n",
    "    # ---\n",
    "    if resize_actions:\n",
    "        target_length=int(action_lengths.mean()*3.5)\n",
    "        actions = _resize_gestures(actions, target_length)\n",
    "        print(f\"Action sequences resized: {target_length=} | {actions.shape=}\")\n",
    "\n",
    "    # ---\n",
    "    l_dataset, n_skeletons, n_coords = actions.shape\n",
    "    actions = actions.reshape(l_dataset, n_skeletons, -1, 3)\n",
    "    print(f\"Action sequences reshaped: {actions.shape=} | {actions[0].shape=}\")\n",
    "    # print(\"Normalized skeleton coordinates: \\n\\t\", actions[0, 0, :3, :])\n",
    "\n",
    "    # ---\n",
    "    o_actions = actions.copy()\n",
    "    x, y, z = 0, 1, 2\n",
    "    o_actions[..., x] = 1280 - (o_actions[..., x] * 2560)\n",
    "    o_actions[..., y] = 960 - (o_actions[..., y] * 1920)\n",
    "    o_actions[..., z] = o_actions[..., z] * 10000 / 7.8125\n",
    "    # print(\"Original skeleton coordinates: \\n\", o_actions[0, 0, :3, :])\n",
    "\n",
    "    # ---\n",
    "    labels = []\n",
    "\n",
    "    for f in files:\n",
    "        a_idx = f.stem.split(\"-\")[2]\n",
    "        a_lbl = f\"{activity_labels[a_idx]}-{f.stem.replace(f'-{a_idx}', '')}\"\n",
    "        labels.append(a_lbl)\n",
    "\n",
    "    print(f\"Action labels generated: {len(labels)=} | {labels[0]=}\\n\")\n",
    "\n",
    "    # ---\n",
    "    assert len(actions) == len(o_actions) == len(labels)\n",
    "    return actions, o_actions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_data(filepath):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        data = pickle.load(f, encoding=\"latin1\")\n",
    "\n",
    "    return (\n",
    "        data[\"X_norm_train\"], data[\"X_norm_valid\"],\n",
    "        data[\"X_orig_train\"], data[\"X_orig_valid\"],\n",
    "        data[\"labels_train\"], data[\"labels_valid\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyright: reportGeneralTypeIssues=false\n",
    "def create_train_valid_data(ds_directory=\"../datasets/SBUKId/\", resize_actions=True, seed=17711):\n",
    "    txts_directory = extract_skeleton_txts(ds_directory, cleanup=True, debug=False)\n",
    "    norm_actions, orig_actions, labels = load_txt_actions(txts_directory, resize_actions)\n",
    "    print(\"> <norm_actions, orig_actions, labels> loaded successfully! \\n\", \n",
    "         f\" {type(norm_actions)=}, {type(orig_actions)=}, {type(labels)=}\"\n",
    "    )\n",
    "    \n",
    "    # ---\n",
    "    (\n",
    "        X_norm_train, X_norm_valid,\n",
    "        X_orig_train, X_orig_valid,\n",
    "        labels_train, labels_valid,\n",
    "    ) = train_test_split(norm_actions, orig_actions, labels, test_size=0.30, random_state=seed)\n",
    "    print(f\"> Training/validation subsets created: \\n\",\n",
    "          f\" {X_norm_train.shape=} | {X_norm_valid.shape=} | {len(labels_valid)=} \\n\",\n",
    "          f\" {type(X_norm_valid)=}, {type(X_orig_valid)=}, {type(labels_valid)=}\",\n",
    "    )\n",
    "    \n",
    "    # ---\n",
    "    ds_directory = Path(ds_directory)\n",
    "    data_path = f\"{ds_directory.parent}/{ds_directory.stem}_3D_dictTVS_s{len(labels)}.pckl\"\n",
    "    data = {\n",
    "        \"X_norm_train\": X_norm_train, \"X_norm_valid\": X_norm_valid,\n",
    "        \"X_orig_train\": X_orig_train, \"X_orig_valid\": X_orig_valid,\n",
    "        \"labels_train\": labels_train, \"labels_valid\": labels_valid,\n",
    "    }\n",
    "\n",
    "    _write_data(data, filepath=data_path)\n",
    "    print(f\"> {ds_directory.stem} TVS train-valid data written to <{data_path}> successfully!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cross_validation_data(ds_directory=\"../datasets/SBUKId/\", resize_actions=True, seed=17711):\n",
    "    txts_directory = extract_skeleton_txts(ds_directory, cleanup=True, debug=False)\n",
    "    norm_actions, orig_actions, labels = load_txt_actions(txts_directory, resize_actions)\n",
    "    print(\"> <norm_actions, orig_actions, labels> loaded successfully! \\n\", \n",
    "         f\" {type(norm_actions)=}, {type(orig_actions)=}, {type(labels)=}\"\n",
    "    )\n",
    "    \n",
    "    # ---\n",
    "    cvs_folds = [\n",
    "        [1, 9, 15, 19],        # Fold 1\n",
    "        [5, 7, 10, 16],        # Fold 2\n",
    "        [2, 3, 20, 21],        # Fold 3\n",
    "        [4, 6, 8, 11],         # Fold 4\n",
    "        [12, 13, 14, 17, 18],  # Fold 5\n",
    "    ]\n",
    "    cvs_data_folds = {\n",
    "        \"f01\" : [[], [], []],\n",
    "        \"f02\" : [[], [], []],\n",
    "        \"f03\" : [[], [], []],\n",
    "        \"f04\" : [[], [], []],\n",
    "        \"f05\" : [[], [], []],\n",
    "    }\n",
    "    ds_directory = Path(ds_directory)\n",
    "    data_path = \"\"\n",
    "\n",
    "    # ---\n",
    "    for (n_action, o_action, lbl) in zip(norm_actions, orig_actions, labels):\n",
    "        _set = int(lbl.split(\"-\")[1].replace(\"st\", \"\"))\n",
    "        \n",
    "        if   _set in cvs_folds[0]: _fold = \"f01\"\n",
    "        elif _set in cvs_folds[1]: _fold = \"f02\"\n",
    "        elif _set in cvs_folds[2]: _fold = \"f03\"\n",
    "        elif _set in cvs_folds[3]: _fold = \"f04\"\n",
    "        else                     : _fold = \"f05\"\n",
    "\n",
    "        cvs_data_folds[_fold][0].append(n_action)\n",
    "        cvs_data_folds[_fold][1].append(o_action)\n",
    "        cvs_data_folds[_fold][2].append(lbl)\n",
    "\n",
    "    # ---\n",
    "    for valid_fold in cvs_data_folds.keys():\n",
    "        X_norm_train, X_norm_valid = [], []\n",
    "        X_orig_train, X_orig_valid = [], []\n",
    "        labels_train, labels_valid = [], []\n",
    "        \n",
    "        # ---\n",
    "        for train_fold in cvs_data_folds.keys():\n",
    "            if (valid_fold == train_fold):\n",
    "                X_norm_valid, X_orig_valid, labels_valid = cvs_data_folds[valid_fold]\n",
    "            else:\n",
    "                X_norm_train.extend(cvs_data_folds[train_fold][0])\n",
    "                X_orig_train.extend(cvs_data_folds[train_fold][1])\n",
    "                labels_train.extend(cvs_data_folds[train_fold][2])\n",
    "\n",
    "        # ---\n",
    "        X_norm_train, X_norm_valid = np.array(X_norm_train), np.array(X_norm_valid)\n",
    "        X_orig_train, X_orig_valid = np.array(X_orig_train), np.array(X_orig_valid)\n",
    "        print(f\"> @Fold {valid_fold[1:]}: Training/validation subsets created: \\n\",\n",
    "            f\" {X_norm_train.shape=} | {X_norm_valid.shape=} | {len(labels_valid)=} \\n\",\n",
    "            f\" {type(X_norm_valid)=}, {type(X_orig_valid)=}, {type(labels_valid)=}\",\n",
    "        )\n",
    "\n",
    "        # ---\n",
    "        data_path = f\"{ds_directory.parent}/{ds_directory.stem}_3D_dictCVS_{valid_fold}_s{len(labels)}.pckl\"\n",
    "        data = {\n",
    "            \"X_norm_train\": X_norm_train, \"X_norm_valid\": X_norm_valid,\n",
    "            \"X_orig_train\": X_orig_train, \"X_orig_valid\": X_orig_valid,\n",
    "            \"labels_train\": labels_train, \"labels_valid\": labels_valid,\n",
    "        }\n",
    "        _write_data(data, filepath=data_path)\n",
    "\n",
    "    # ---\n",
    "    print(f\"> {ds_directory.stem} CVS train-valid data folds written to <{data_path.replace('05', '*')}> successfully!\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<l_dataset=282> skeleton text files extracted to <..\\datasets\\SBUKId.txts> dataset directory.\n",
      "\n",
      "action_lengths.min()=10, action_lengths.mean()=24.2, action_lengths.max()=46\n",
      "Action sequences resized: target_length=84 | actions.shape=(282, 84, 90)\n",
      "Action sequences reshaped: actions.shape=(282, 84, 30, 3) | actions[0].shape=(84, 30, 3)\n",
      "Action labels generated: len(labels)=282 | labels[0]='1.Approaching-st01-p1p2-sq01'\n",
      "\n",
      "> <norm_actions, orig_actions, labels> loaded successfully! \n",
      "  type(norm_actions)=<class 'numpy.ndarray'>, type(orig_actions)=<class 'numpy.ndarray'>, type(labels)=<class 'list'>\n",
      "> @Fold 01: Training/validation subsets created: \n",
      "  X_norm_train.shape=(227, 84, 30, 3) | X_norm_valid.shape=(55, 84, 30, 3) | len(labels_valid)=55 \n",
      "  type(X_norm_valid)=<class 'numpy.ndarray'>, type(X_orig_valid)=<class 'numpy.ndarray'>, type(labels_valid)=<class 'list'>\n",
      "> @Fold 02: Training/validation subsets created: \n",
      "  X_norm_train.shape=(230, 84, 30, 3) | X_norm_valid.shape=(52, 84, 30, 3) | len(labels_valid)=52 \n",
      "  type(X_norm_valid)=<class 'numpy.ndarray'>, type(X_orig_valid)=<class 'numpy.ndarray'>, type(labels_valid)=<class 'list'>\n",
      "> @Fold 03: Training/validation subsets created: \n",
      "  X_norm_train.shape=(226, 84, 30, 3) | X_norm_valid.shape=(56, 84, 30, 3) | len(labels_valid)=56 \n",
      "  type(X_norm_valid)=<class 'numpy.ndarray'>, type(X_orig_valid)=<class 'numpy.ndarray'>, type(labels_valid)=<class 'list'>\n",
      "> @Fold 04: Training/validation subsets created: \n",
      "  X_norm_train.shape=(228, 84, 30, 3) | X_norm_valid.shape=(54, 84, 30, 3) | len(labels_valid)=54 \n",
      "  type(X_norm_valid)=<class 'numpy.ndarray'>, type(X_orig_valid)=<class 'numpy.ndarray'>, type(labels_valid)=<class 'list'>\n",
      "> @Fold 05: Training/validation subsets created: \n",
      "  X_norm_train.shape=(217, 84, 30, 3) | X_norm_valid.shape=(65, 84, 30, 3) | len(labels_valid)=65 \n",
      "  type(X_norm_valid)=<class 'numpy.ndarray'>, type(X_orig_valid)=<class 'numpy.ndarray'>, type(labels_valid)=<class 'list'>\n",
      "> SBUKId CVS train-valid data folds written to <..\\datasets/SBUKId_3D_dictCVS_f*_s282.pckl> successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_cross_validation_data(ds_directory=\"../datasets/SBUKId/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd523941f93840f3066dc30de95cef5f0b3787e3e58410dcba83902e24288dc0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
