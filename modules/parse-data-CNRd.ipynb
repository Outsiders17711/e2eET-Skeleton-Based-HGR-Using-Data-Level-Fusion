{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## e2eET Skeleton Based HGR Using Data-Level Fusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2N5i0LEcxRtT"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import numpy as np\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def do_train_valid_split(root, valid_pct=0.3, save_path=None, verbose=False):\n",
        "    \n",
        "    # iterate over root directory and init random state\n",
        "    ds = np.array(list(Path(root).rglob(\"*.png\")), dtype=object)\n",
        "    rng = np.random.default_rng(17711)\n",
        "\n",
        "    # partition the original dataset train/valid subsets\n",
        "    l_train = len(ds) - int(len(ds) * (valid_pct))\n",
        "    ds_subsets = np.ones_like(ds, dtype=object)\n",
        "\n",
        "    # initialize the new dataset directory and subsets\n",
        "    if save_path is None: save_path = Path(\"../images_d/CNR-3d-original-1920px.1080px-[topdown]/\")\n",
        "    ds_subsets[:l_train] = save_path.joinpath(\"train\")\n",
        "    ds_subsets[l_train:] = save_path.joinpath(\"valid\")\n",
        "    if verbose: print(f\"{l_train=} | l_valid={(len(ds) - l_train)}\\n{ds[:3]=}\")\n",
        "\n",
        "    # shuffle the original dataset, twice\n",
        "    rng.shuffle(ds) ; rng.shuffle(ds)\n",
        "    if verbose: print(f\"{ds[:3]=}\")\n",
        "\n",
        "    # copy files from the original dataset to the new dataset subset (rename to work with dataloader)\n",
        "    for f, subset in zip(ds, ds_subsets):\n",
        "        n_f = subset.joinpath(f.relative_to(root))\n",
        "        n_f = Path(str(n_f).replace(\".png\", \"/top-down.png\"))\n",
        "        if verbose: print(f\"\\n old: {f=} \\n -> new: {n_f=} \\n\")\n",
        "\n",
        "        n_f.parent.mkdir(parents=True, exist_ok=True)\n",
        "        shutil.copy(f, n_f)\n",
        "        if verbose: break\n",
        "\n",
        "    # print new dataset information\n",
        "    for d in Path(save_path).iterdir():\n",
        "        print(f\"@{d.name} subset: {len(list(d.iterdir()))} classes | {len(list(d.rglob('*.png')))} gesture sequences\")\n",
        "\n",
        "# do_train_valid_split(root=\"../datasets/CNR\", valid_pct=0.3, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "@train subset: 16 classes | 1348 gesture sequences\n",
            "@valid subset: 16 classes | 577 gesture sequences\n"
          ]
        }
      ],
      "source": [
        "do_train_valid_split(root=\"../datasets/CNR\", valid_pct=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "deep_learning_hand_gesture_recognition_01_data_download.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 ('hlu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "fd523941f93840f3066dc30de95cef5f0b3787e3e58410dcba83902e24288dc0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
